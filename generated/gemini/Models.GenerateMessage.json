{
  "moduleName": "Gemini / Models / Generate Message",
  "version": "0.0.0",
  "description": "Generates a response from the model given an input `MessagePrompt`.",
  "keywords": [
    "ai",
    "google"
  ],
  "cacheMode": "always",
  "evalMode": "manual",
  "params": {
    "auth": {
      "schema": {
        "type": "string",
        "description": "Gemini API key.\n"
      },
      "advanced": false,
      "hideValue": false
    },
    "$Xgafv": {
      "schema": {
        "type": "string",
        "enum": [
          "1",
          "2"
        ],
        "optional": true,
        "description": "V1 error format."
      },
      "advanced": true,
      "hideValue": false
    },
    "accessToken": {
      "schema": {
        "type": "string",
        "optional": true,
        "description": "OAuth access token."
      },
      "advanced": true,
      "hideValue": false
    },
    "alt": {
      "schema": {
        "type": "string",
        "enum": [
          "json",
          "media",
          "proto"
        ],
        "optional": true,
        "description": "Data format for response."
      },
      "advanced": true,
      "hideValue": false
    },
    "callback": {
      "schema": {
        "type": "string",
        "optional": true,
        "description": "JSONP"
      },
      "advanced": true,
      "hideValue": false
    },
    "fields": {
      "schema": {
        "type": "string",
        "optional": true,
        "description": "Selector specifying which fields to include in a partial response."
      },
      "advanced": true,
      "hideValue": false
    },
    "key": {
      "schema": {
        "type": "string",
        "optional": true,
        "description": "API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token."
      },
      "advanced": true,
      "hideValue": false
    },
    "oauthToken": {
      "schema": {
        "type": "string",
        "optional": true,
        "description": "OAuth 2.0 token for the current user."
      },
      "advanced": true,
      "hideValue": false
    },
    "prettyPrint": {
      "schema": {
        "type": "boolean",
        "optional": true,
        "description": "Returns response with indentations and line breaks."
      },
      "advanced": true,
      "hideValue": false
    },
    "quotaUser": {
      "schema": {
        "type": "string",
        "optional": true,
        "description": "Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters."
      },
      "advanced": true,
      "hideValue": false
    },
    "uploadProtocol": {
      "schema": {
        "type": "string",
        "optional": true,
        "description": "Upload protocol for media (e.g. \"raw\", \"multipart\")."
      },
      "advanced": true,
      "hideValue": false
    },
    "uploadType": {
      "schema": {
        "type": "string",
        "optional": true,
        "description": "Legacy upload protocol for media (e.g. \"media\", \"multipart\")."
      },
      "advanced": true,
      "hideValue": false
    },
    "model": {
      "schema": {
        "type": "string",
        "description": "Required. The name of the model to use. Format: `name=models/{model}`."
      },
      "advanced": false,
      "hideValue": false
    },
    "candidateCount": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. The number of generated response messages to return. This value must be between `[1, 8]`, inclusive. If unset, this will default to `1`."
      },
      "advanced": true,
      "hideValue": false
    },
    "prompt": {
      "schema": {
        "type": "any",
        "optional": true,
        "description": "All of the structured input text passed to the model as a prompt. A `MessagePrompt` contains a structured set of fields that provide context for the conversation, examples of user input/model output message pairs that prime the model to respond in different ways, and the conversation history or list of messages representing the alternating turns of the conversation between the user and the model."
      },
      "advanced": true,
      "hideValue": true
    },
    "temperature": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. Controls the randomness of the output. Values can range over `[0.0,1.0]`, inclusive. A value closer to `1.0` will produce responses that are more varied, while a value closer to `0.0` will typically result in less surprising responses from the model."
      },
      "advanced": true,
      "hideValue": false
    },
    "topK": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. The maximum number of tokens to consider when sampling. The model uses combined Top-k and nucleus sampling. Top-k sampling considers the set of `top_k` most probable tokens."
      },
      "advanced": true,
      "hideValue": false
    },
    "topP": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. The maximum cumulative probability of tokens to consider when sampling. The model uses combined Top-k and nucleus sampling. Nucleus sampling considers the smallest set of tokens whose probability sum is at least `top_p`."
      },
      "advanced": true,
      "hideValue": false
    }
  },
  "result": {
    "schema": {
      "type": "any"
    },
    "async": true
  },
  "attributes": {
    "externalDocs": "",
    "codeHash": "feba97df620e3882666e3006d8dff4f23c86f4672d633b7eb3a0a21f91049266"
  }
}