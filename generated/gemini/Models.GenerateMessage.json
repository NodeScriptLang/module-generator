{
  "moduleName": "Gemini / Models / Generate Message",
  "version": "0.0.0",
  "description": "Generates a response from the model given an input `MessagePrompt`.",
  "keywords": [
    "ai",
    "google"
  ],
  "cacheMode": "always",
  "evalMode": "manual",
  "params": {
    "accessToken": {
      "schema": {
        "type": "string",
        "optional": true,
        "description": "OAuth 2.0 token for the current user."
      },
      "advanced": true,
      "hideValue": false
    },
    "apiKey": {
      "schema": {
        "type": "string",
        "optional": true,
        "description": "API key. Your API key identifies your project and provides you with\nAPI access, quota, and reports. Required unless you provide an OAuth 2.0 token.\n"
      },
      "advanced": true,
      "hideValue": false
    },
    "model": {
      "schema": {
        "type": "string",
        "description": "Required. The name of the model to use. Format: `name=models/{model}`."
      },
      "advanced": false,
      "hideValue": false
    },
    "candidateCount": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. The number of generated response messages to return. This value must be between `[1, 8]`, inclusive. If unset, this will default to `1`."
      },
      "advanced": true,
      "hideValue": false
    },
    "prompt": {
      "schema": {
        "type": "any",
        "optional": true,
        "description": "All of the structured input text passed to the model as a prompt. A `MessagePrompt` contains a structured set of fields that provide context for the conversation, examples of user input/model output message pairs that prime the model to respond in different ways, and the conversation history or list of messages representing the alternating turns of the conversation between the user and the model."
      },
      "advanced": true,
      "hideValue": true
    },
    "temperature": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. Controls the randomness of the output. Values can range over `[0.0,1.0]`, inclusive. A value closer to `1.0` will produce responses that are more varied, while a value closer to `0.0` will typically result in less surprising responses from the model."
      },
      "advanced": true,
      "hideValue": false
    },
    "topK": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. The maximum number of tokens to consider when sampling. The model uses combined Top-k and nucleus sampling. Top-k sampling considers the set of `top_k` most probable tokens."
      },
      "advanced": true,
      "hideValue": false
    },
    "topP": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. The maximum cumulative probability of tokens to consider when sampling. The model uses combined Top-k and nucleus sampling. Nucleus sampling considers the smallest set of tokens whose probability sum is at least `top_p`."
      },
      "advanced": true,
      "hideValue": false
    }
  },
  "result": {
    "schema": {
      "type": "any"
    },
    "async": true
  },
  "attributes": {
    "externalDocs": "",
    "codeHash": "fa8e6e4c1eababb6b29630fd85e66697855f01c77b0e51c4989a1487733b1324"
  }
}