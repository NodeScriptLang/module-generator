{
  "moduleName": "Gemini / Models / Generate Text",
  "version": "0.0.0",
  "description": "Generates a response from the model given an input message.",
  "keywords": [
    "ai",
    "google"
  ],
  "cacheMode": "always",
  "evalMode": "manual",
  "params": {
    "accessToken": {
      "schema": {
        "type": "string",
        "optional": true,
        "description": "OAuth 2.0 token for the current user."
      },
      "advanced": true,
      "hideValue": false
    },
    "apiKey": {
      "schema": {
        "type": "string",
        "optional": true,
        "description": "API key. Your API key identifies your project and provides you with\nAPI access, quota, and reports. Required unless you provide an OAuth 2.0 token.\n"
      },
      "advanced": true,
      "hideValue": false
    },
    "model": {
      "schema": {
        "type": "string",
        "description": "Required. The name of the `Model` or `TunedModel` to use for generating the completion. Examples: models/text-bison-001 tunedModels/sentence-translator-u3b7m"
      },
      "advanced": false,
      "hideValue": false
    },
    "candidateCount": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. Number of generated responses to return. This value must be between [1, 8], inclusive. If unset, this will default to 1."
      },
      "advanced": true,
      "hideValue": false
    },
    "maxOutputTokens": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. The maximum number of tokens to include in a candidate. If unset, this will default to output_token_limit specified in the `Model` specification."
      },
      "advanced": true,
      "hideValue": false
    },
    "prompt": {
      "schema": {
        "type": "any",
        "optional": true,
        "description": "Text given to the model as a prompt. The Model will use this TextPrompt to Generate a text completion."
      },
      "advanced": true,
      "hideValue": true
    },
    "safetySettings": {
      "schema": {
        "type": "array",
        "items": {
          "type": "any"
        },
        "optional": true,
        "description": "Optional. A list of unique `SafetySetting` instances for blocking unsafe content. that will be enforced on the `GenerateTextRequest.prompt` and `GenerateTextResponse.candidates`. There should not be more than one setting for each `SafetyCategory` type. The API will block any prompts and responses that fail to meet the thresholds set by these settings. This list overrides the default settings for each `SafetyCategory` specified in the safety_settings. If there is no `SafetySetting` for a given `SafetyCategory` provided in the list, the API will use the default safety setting for that category. Harm categories HARM_CATEGORY_DEROGATORY, HARM_CATEGORY_TOXICITY, HARM_CATEGORY_VIOLENCE, HARM_CATEGORY_SEXUAL, HARM_CATEGORY_MEDICAL, HARM_CATEGORY_DANGEROUS are supported in text service."
      },
      "advanced": true,
      "hideValue": false
    },
    "stopSequences": {
      "schema": {
        "type": "array",
        "items": {
          "type": "any"
        },
        "optional": true,
        "description": "The set of character sequences (up to 5) that will stop output generation. If specified, the API will stop at the first appearance of a stop sequence. The stop sequence will not be included as part of the response."
      },
      "advanced": true,
      "hideValue": false
    },
    "temperature": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. Controls the randomness of the output. Note: The default value varies by model, see the `Model.temperature` attribute of the `Model` returned the `getModel` function. Values can range from [0.0,1.0], inclusive. A value closer to 1.0 will produce responses that are more varied and creative, while a value closer to 0.0 will typically result in more straightforward responses from the model."
      },
      "advanced": true,
      "hideValue": false
    },
    "topK": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. The maximum number of tokens to consider when sampling. The model uses combined Top-k and nucleus sampling. Top-k sampling considers the set of `top_k` most probable tokens. Defaults to 40. Note: The default value varies by model, see the `Model.top_k` attribute of the `Model` returned the `getModel` function."
      },
      "advanced": true,
      "hideValue": false
    },
    "topP": {
      "schema": {
        "type": "number",
        "optional": true,
        "description": "Optional. The maximum cumulative probability of tokens to consider when sampling. The model uses combined Top-k and nucleus sampling. Tokens are sorted based on their assigned probabilities so that only the most likely tokens are considered. Top-k sampling directly limits the maximum number of tokens to consider, while Nucleus sampling limits number of tokens based on the cumulative probability. Note: The default value varies by model, see the `Model.top_p` attribute of the `Model` returned the `getModel` function."
      },
      "advanced": true,
      "hideValue": false
    }
  },
  "result": {
    "schema": {
      "type": "any"
    },
    "async": true
  },
  "attributes": {
    "externalDocs": "",
    "codeHash": "415a81963222406fcab9e5fdbcac3e446f2c3dc98842d64b7ca69f65378dce3a"
  }
}