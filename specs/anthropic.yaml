id: anthropic
displayName: Anthropic
baseUrl: https://api.anthropic.com
description: The Anthropic REST API. Please see https://api.anthropic.com for more details.
commonKeywords:
  - ai
  - claude
commonParams:
  - paramName: apiKey
    paramKey: x-api-key
    prefix: Bearer
    description: |
      Your unique API key for authentication. 
      This key is required in the header of all API requests, to
      authenticate your account and access Anthropic's services. Get your
      API key through the
      [Console](https://console.anthropic.com/settings/keys). Each key is
      scoped to a Workspace.
    in: header
    schema:
      type: string
    required: false
modules:
  - moduleName: Messages / Create
    method: post
    path: /v1/messages
    description: >-
      Send a structured list of input messages with text and/or image content,
      and the model will generate the next message in the conversation.


      The Messages API can be used for either single queries or stateless
      multi-turn conversations.
    externalDocs: ""
    params:
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
      - paramName: model
        description: The model that will complete your prompt.\n\nSee
          [models](https://docs.anthropic.com/en/docs/models-overview) for
          additional details and options.
        paramKey: model
        in: body
        schema:
          type: any
        required: true
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: messages
        description: >-
          Input messages.


          Our models are trained to operate on alternating `user` and
          `assistant` conversational turns. When creating a new `Message`, you
          specify the prior conversational turns with the `messages` parameter,
          and the model then generates the next `Message` in the conversation.
          Consecutive `user` or `assistant` turns in your request will be
          combined into a single turn.


          Each input message must be an object with a `role` and `content`. You
          can specify a single `user`-role message, or you can include multiple
          `user` and `assistant` messages.


          If the final message uses the `assistant` role, the response content
          will continue immediately from the content in that message. This can
          be used to constrain part of the model's response.


          Example with a single `user` message:


          ```json

          [{"role": "user", "content": "Hello, Claude"}]

          ```


          Example with multiple conversational turns:


          ```json

          [
            {"role": "user", "content": "Hello there."},
            {"role": "assistant", "content": "Hi, I'm Claude. How can I help you?"},
            {"role": "user", "content": "Can you explain LLMs in plain English?"},
          ]

          ```


          Example with a partially-filled response from Claude:


          ```json

          [
            {"role": "user", "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"},
            {"role": "assistant", "content": "The best answer is ("},
          ]

          ```


          Each input message `content` may be either a single `string` or an
          array of content blocks, where each block has a specific `type`. Using
          a `string` for `content` is shorthand for an array of one content
          block of type `"text"`. The following input messages are equivalent:


          ```json

          {"role": "user", "content": "Hello, Claude"}

          ```


          ```json

          {"role": "user", "content": [{"type": "text", "text": "Hello,
          Claude"}]}

          ```


          Starting with Claude 3 models, you can also send image content blocks:


          ```json

          {"role": "user", "content": [
            {
              "type": "image",
              "source": {
                "type": "base64",
                "media_type": "image/jpeg",
                "data": "/9j/4AAQSkZJRg...",
              }
            },
            {"type": "text", "text": "What is in this image?"}
          ]}

          ```


          We currently support the `base64` source type for images, and the
          `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types.


          See
          [examples](https://docs.anthropic.com/en/api/messages-examples#vision)
          for more input examples.


          Note that if you want to include a [system
          prompt](https://docs.anthropic.com/en/docs/system-prompts), you can
          use the top-level `system` parameter — there is no `"system"` role for
          input messages in the Messages API.
        paramKey: messages
        in: body
        schema:
          type: array
          items:
            type: any
        required: true
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: maxTokens
        description: >-
          The maximum number of tokens to generate before stopping.


          Note that our models may stop _before_ reaching this maximum. This
          parameter only specifies the absolute maximum number of tokens to
          generate.


          Different models have different maximum values for this
          parameter.  See
          [models](https://docs.anthropic.com/en/docs/models-overview) for
          details.
        paramKey: max_tokens
        in: body
        schema:
          type: number
          minimum: 1
        required: true
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: metadata
        description: An object describing metadata about the request.
        paramKey: metadata
        in: body
        schema:
          type: any
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: stopSequences
        description: >-
          Custom text sequences that will cause the model to stop generating.


          Our models will normally stop when they have naturally completed their
          turn, which will result in a response `stop_reason` of `"end_turn"`.


          If you want the model to stop generating when it encounters custom
          strings of text, you can use the `stop_sequences` parameter. If the
          model encounters one of the custom sequences, the response
          `stop_reason` value will be `"stop_sequence"` and the response
          `stop_sequence` value will contain the matched stop sequence.
        paramKey: stop_sequences
        in: body
        schema:
          type: array
          items:
            type: any
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: stream
        description: >-
          Whether to incrementally stream the response using server-sent events.


          See [streaming](https://docs.anthropic.com/en/api/messages-streaming)
          for details.
        paramKey: stream
        in: body
        schema:
          type: boolean
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: system
        description: >-
          System prompt.


          A system prompt is a way of providing context and instructions to
          Claude, such as specifying a particular goal or role. See our [guide
          to system prompts](https://docs.anthropic.com/en/docs/system-prompts).
        paramKey: system
        in: body
        schema:
          type: any
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: temperature
        description: >-
          Amount of randomness injected into the response.


          Defaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature`
          closer to `0.0` for analytical / multiple choice, and closer to `1.0`
          for creative and generative tasks.


          Note that even with `temperature` of `0.0`, the results will not be
          fully deterministic.
        paramKey: temperature
        in: body
        schema:
          type: number
          minimum: 0
          maximum: 1
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: toolChoice
        description: How the model should use the provided tools. The model can use a
          specific tool, any available tool, or decide by itself.
        paramKey: tool_choice
        in: body
        schema:
          type: any
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: tools
        description: >-
          Definitions of tools that the model may use.


          If you include `tools` in your API request, the model may return
          `tool_use` content blocks that represent the model's use of those
          tools. You can then run those tools using the tool input generated by
          the model and then optionally return results back to the model using
          `tool_result` content blocks.


          Each tool definition includes:


          * `name`: Name of the tool.

          * `description`: Optional, but strongly-recommended description of the
          tool.

          * `input_schema`: [JSON schema](https://json-schema.org/) for the tool
          `input` shape that the model will produce in `tool_use` output content
          blocks.


          For example, if you defined `tools` as:


          ```json

          [
            {
              "name": "get_stock_price",
              "description": "Get the current stock price for a given ticker symbol.",
              "input_schema": {
                "type": "object",
                "properties": {
                  "ticker": {
                    "type": "string",
                    "description": "The stock ticker symbol, e.g. AAPL for Apple Inc."
                  }
                },
                "required": ["ticker"]
              }
            }
          ]

          ```


          And then asked the model "What's the S&P 500 at today?", the model
          might produce `tool_use` content blocks in the response like this:


          ```json

          [
            {
              "type": "tool_use",
              "id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
              "name": "get_stock_price",
              "input": { "ticker": "^GSPC" }
            }
          ]

          ```


          You might then run your `get_stock_price` tool with `{"ticker":
          "^GSPC"}` as an input, and return the following back to the model in a
          subsequent `user` message:


          ```json

          [
            {
              "type": "tool_result",
              "tool_use_id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
              "content": "259.75 USD"
            }
          ]

          ```


          Tools can be used for workflows that include running client-side tools
          and functions, or more generally whenever you want the model to
          produce a particular JSON structure of output.


          See our [guide](https://docs.anthropic.com/en/docs/tool-use) for more
          details.
        paramKey: tools
        in: body
        schema:
          type: array
          items:
            type: any
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: topK
        description: >-
          Only sample from the top K options for each subsequent token.


          Used to remove "long tail" low probability responses. [Learn more
          technical details
          here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).


          Recommended for advanced use cases only. You usually only need to use
          `temperature`.
        paramKey: top_k
        in: body
        schema:
          type: number
          minimum: 0
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: topP
        description: >-
          Use nucleus sampling.


          In nucleus sampling, we compute the cumulative distribution over all
          the options for each subsequent token in decreasing probability order
          and cut it off once it reaches a particular probability specified by
          `top_p`. You should either alter `temperature` or `top_p`, but not
          both.


          Recommended for advanced use cases only. You usually only need to use
          `temperature`.
        paramKey: top_p
        in: body
        schema:
          type: number
          minimum: 0
          maximum: 1
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
    requestBodyType: json
  - moduleName: Text Completions / Complete
    method: post
    path: /v1/complete
    description: >-
      [Legacy] Create a Text Completion.


      The Text Completions API is a legacy API. We recommend using the [Messages
      API](https://docs.anthropic.com/en/api/messages) going forward.


      Future models and features will not be compatible with Text Completions.
      See our [migration
      guide](https://docs.anthropic.com/en/api/migrating-from-text-completions-to-messages)
      for guidance in migrating from Text Completions to Messages.
    externalDocs: ""
    params:
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
      - paramName: model
        description: The model that will complete your prompt.\n\nSee
          [models](https://docs.anthropic.com/en/docs/models-overview) for
          additional details and options.
        paramKey: model
        in: body
        schema:
          type: any
        required: true
        example:
          model: claude-2.1
          prompt: |-
            

            Human: Hello, world!

            Assistant:
          max_tokens_to_sample: 256
      - paramName: prompt
        description: >-
          The prompt that you want Claude to complete.


          For proper response generation you will need to format your prompt
          using alternating `\n\nHuman:` and `\n\nAssistant:` conversational
          turns. For example:


          ```

          "\n\nHuman: {userQuestion}\n\nAssistant:"

          ```


          See [prompt
          validation](https://docs.anthropic.com/en/api/prompt-validation) and
          our guide to [prompt
          design](https://docs.anthropic.com/en/docs/intro-to-prompting) for
          more details.
        paramKey: prompt
        in: body
        schema:
          type: string
        required: true
        example:
          model: claude-2.1
          prompt: |-
            

            Human: Hello, world!

            Assistant:
          max_tokens_to_sample: 256
      - paramName: maxTokensToSample
        description: >-
          The maximum number of tokens to generate before stopping.


          Note that our models may stop _before_ reaching this maximum. This
          parameter only specifies the absolute maximum number of tokens to
          generate.
        paramKey: max_tokens_to_sample
        in: body
        schema:
          type: number
          minimum: 1
        required: true
        example:
          model: claude-2.1
          prompt: |-
            

            Human: Hello, world!

            Assistant:
          max_tokens_to_sample: 256
      - paramName: stopSequences
        description: >-
          Sequences that will cause the model to stop generating.


          Our models stop on `"\n\nHuman:"`, and may include additional built-in
          stop sequences in the future. By providing the stop_sequences
          parameter, you may include additional strings that will cause the
          model to stop generating.
        paramKey: stop_sequences
        in: body
        schema:
          type: array
          items:
            type: any
        required: false
        example:
          model: claude-2.1
          prompt: |-
            

            Human: Hello, world!

            Assistant:
          max_tokens_to_sample: 256
      - paramName: temperature
        description: >-
          Amount of randomness injected into the response.


          Defaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature`
          closer to `0.0` for analytical / multiple choice, and closer to `1.0`
          for creative and generative tasks.


          Note that even with `temperature` of `0.0`, the results will not be
          fully deterministic.
        paramKey: temperature
        in: body
        schema:
          type: number
          minimum: 0
          maximum: 1
        required: false
        example:
          model: claude-2.1
          prompt: |-
            

            Human: Hello, world!

            Assistant:
          max_tokens_to_sample: 256
      - paramName: topP
        description: >-
          Use nucleus sampling.


          In nucleus sampling, we compute the cumulative distribution over all
          the options for each subsequent token in decreasing probability order
          and cut it off once it reaches a particular probability specified by
          `top_p`. You should either alter `temperature` or `top_p`, but not
          both.


          Recommended for advanced use cases only. You usually only need to use
          `temperature`.
        paramKey: top_p
        in: body
        schema:
          type: number
          minimum: 0
          maximum: 1
        required: false
        example:
          model: claude-2.1
          prompt: |-
            

            Human: Hello, world!

            Assistant:
          max_tokens_to_sample: 256
      - paramName: topK
        description: >-
          Only sample from the top K options for each subsequent token.


          Used to remove "long tail" low probability responses. [Learn more
          technical details
          here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).


          Recommended for advanced use cases only. You usually only need to use
          `temperature`.
        paramKey: top_k
        in: body
        schema:
          type: number
          minimum: 0
        required: false
        example:
          model: claude-2.1
          prompt: |-
            

            Human: Hello, world!

            Assistant:
          max_tokens_to_sample: 256
      - paramName: metadata
        description: An object describing metadata about the request.
        paramKey: metadata
        in: body
        schema:
          type: any
        required: false
        example:
          model: claude-2.1
          prompt: |-
            

            Human: Hello, world!

            Assistant:
          max_tokens_to_sample: 256
      - paramName: stream
        description: >-
          Whether to incrementally stream the response using server-sent events.


          See [streaming](https://docs.anthropic.com/en/api/streaming) for
          details.
        paramKey: stream
        in: body
        schema:
          type: boolean
        required: false
        example:
          model: claude-2.1
          prompt: |-
            

            Human: Hello, world!

            Assistant:
          max_tokens_to_sample: 256
    requestBodyType: json
  - moduleName: Models / List
    method: get
    path: /v1/models
    description: >-
      List available models.


      The Models API response can be used to determine which models are
      available for use in the API. More recently released models are listed
      first.
    externalDocs: ""
    params:
      - paramName: beforeId
        description: ID of the object to use as a cursor for pagination. When provided,
          returns the page of results immediately before this object.
        paramKey: before_id
        in: query
        schema:
          type: string
        required: false
      - paramName: afterId
        description: ID of the object to use as a cursor for pagination. When provided,
          returns the page of results immediately after this object.
        paramKey: after_id
        in: query
        schema:
          type: string
        required: false
      - paramName: limit
        description: |-
          Number of items to return per page.

          Defaults to `20`. Ranges from `1` to `1000`.
        paramKey: limit
        in: query
        schema:
          type: number
          minimum: 1
          maximum: 1000
          default: 20
        required: false
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Models / Get
    method: get
    path: /v1/models/{model_id}
    description: >-
      Get a specific model.


      The Models API response can be used to determine information about a
      specific model or resolve a model alias to a model ID.
    externalDocs: ""
    params:
      - paramName: modelId
        description: Model identifier or alias.
        paramKey: model_id
        in: path
        schema:
          type: string
        required: true
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Message Batches / Create
    method: post
    path: /v1/messages/batches
    description: >-
      Send a batch of Message creation requests.


      The Message Batches API can be used to process multiple Messages API
      requests at once. Once a Message Batch is created, it begins processing
      immediately. Batches can take up to 24 hours to complete.
    externalDocs: ""
    params:
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
      - paramName: requests
        description: List of requests for prompt completion. Each is an individual
          request to create a Message.
        paramKey: requests
        in: body
        schema:
          type: array
          items:
            type: any
        required: true
    requestBodyType: json
  - moduleName: Message Batches / List
    method: get
    path: /v1/messages/batches
    description: List all Message Batches within a Workspace. Most recently created
      batches are returned first.
    externalDocs: ""
    params:
      - paramName: beforeId
        description: ID of the object to use as a cursor for pagination. When provided,
          returns the page of results immediately before this object.
        paramKey: before_id
        in: query
        schema:
          type: string
        required: false
      - paramName: afterId
        description: ID of the object to use as a cursor for pagination. When provided,
          returns the page of results immediately after this object.
        paramKey: after_id
        in: query
        schema:
          type: string
        required: false
      - paramName: limit
        description: |-
          Number of items to return per page.

          Defaults to `20`. Ranges from `1` to `1000`.
        paramKey: limit
        in: query
        schema:
          type: number
          minimum: 1
          maximum: 1000
          default: 20
        required: false
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Message Batches / Retrieve
    method: get
    path: /v1/messages/batches/{message_batch_id}
    description: This endpoint is idempotent and can be used to poll for Message
      Batch completion. To access the results of a Message Batch, make a request
      to the `results_url` field in the response.
    externalDocs: ""
    params:
      - paramName: messageBatchId
        description: ID of the Message Batch.
        paramKey: message_batch_id
        in: path
        schema:
          type: string
        required: true
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Message Batches / Delete
    method: delete
    path: /v1/messages/batches/{message_batch_id}
    description: >-
      Delete a Message Batch.


      Message Batches can only be deleted once they've finished processing. If
      you'd like to delete an in-progress batch, you must first cancel it.
    externalDocs: ""
    params:
      - paramName: messageBatchId
        description: ID of the Message Batch.
        paramKey: message_batch_id
        in: path
        schema:
          type: string
        required: true
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Message Batches / Cancel
    method: post
    path: /v1/messages/batches/{message_batch_id}/cancel
    description: >-
      Batches may be canceled any time before processing ends. Once cancellation
      is initiated, the batch enters a `canceling` state, at which time the
      system may complete any in-progress, non-interruptible requests before
      finalizing cancellation.


      The number of canceled requests is specified in `request_counts`. To
      determine which requests were canceled, check the individual results
      within the batch. Note that cancellation may not result in any canceled
      requests if they were non-interruptible.
    externalDocs: ""
    params:
      - paramName: messageBatchId
        description: ID of the Message Batch.
        paramKey: message_batch_id
        in: path
        schema:
          type: string
        required: true
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Message Batches / Get Results
    method: get
    path: /v1/messages/batches/{message_batch_id}/results
    description: >-
      Streams the results of a Message Batch as a `.jsonl` file.


      Each line in the file is a JSON object containing the result of a single
      request in the Message Batch. Results are not guaranteed to be in the same
      order as requests. Use the `custom_id` field to match results to requests.
    externalDocs: ""
    params:
      - paramName: messageBatchId
        description: ID of the Message Batch.
        paramKey: message_batch_id
        in: path
        schema:
          type: string
        required: true
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Messages / Count Tokens
    method: post
    path: /v1/messages/count_tokens
    description: >-
      Count the number of tokens in a Message.


      The Token Count API can be used to count the number of tokens in a
      Message, including tools, images, and documents, without creating it.
    externalDocs: ""
    params:
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
      - paramName: toolChoice
        description: How the model should use the provided tools. The model can use a
          specific tool, any available tool, or decide by itself.
        paramKey: tool_choice
        in: body
        schema:
          type: any
        required: false
        example:
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: tools
        description: >-
          Definitions of tools that the model may use.


          If you include `tools` in your API request, the model may return
          `tool_use` content blocks that represent the model's use of those
          tools. You can then run those tools using the tool input generated by
          the model and then optionally return results back to the model using
          `tool_result` content blocks.


          Each tool definition includes:


          * `name`: Name of the tool.

          * `description`: Optional, but strongly-recommended description of the
          tool.

          * `input_schema`: [JSON schema](https://json-schema.org/) for the tool
          `input` shape that the model will produce in `tool_use` output content
          blocks.


          For example, if you defined `tools` as:


          ```json

          [
            {
              "name": "get_stock_price",
              "description": "Get the current stock price for a given ticker symbol.",
              "input_schema": {
                "type": "object",
                "properties": {
                  "ticker": {
                    "type": "string",
                    "description": "The stock ticker symbol, e.g. AAPL for Apple Inc."
                  }
                },
                "required": ["ticker"]
              }
            }
          ]

          ```


          And then asked the model "What's the S&P 500 at today?", the model
          might produce `tool_use` content blocks in the response like this:


          ```json

          [
            {
              "type": "tool_use",
              "id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
              "name": "get_stock_price",
              "input": { "ticker": "^GSPC" }
            }
          ]

          ```


          You might then run your `get_stock_price` tool with `{"ticker":
          "^GSPC"}` as an input, and return the following back to the model in a
          subsequent `user` message:


          ```json

          [
            {
              "type": "tool_result",
              "tool_use_id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
              "content": "259.75 USD"
            }
          ]

          ```


          Tools can be used for workflows that include running client-side tools
          and functions, or more generally whenever you want the model to
          produce a particular JSON structure of output.


          See our [guide](https://docs.anthropic.com/en/docs/tool-use) for more
          details.
        paramKey: tools
        in: body
        schema:
          type: array
          items:
            type: any
        required: false
        example:
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: messages
        description: >-
          Input messages.


          Our models are trained to operate on alternating `user` and
          `assistant` conversational turns. When creating a new `Message`, you
          specify the prior conversational turns with the `messages` parameter,
          and the model then generates the next `Message` in the conversation.
          Consecutive `user` or `assistant` turns in your request will be
          combined into a single turn.


          Each input message must be an object with a `role` and `content`. You
          can specify a single `user`-role message, or you can include multiple
          `user` and `assistant` messages.


          If the final message uses the `assistant` role, the response content
          will continue immediately from the content in that message. This can
          be used to constrain part of the model's response.


          Example with a single `user` message:


          ```json

          [{"role": "user", "content": "Hello, Claude"}]

          ```


          Example with multiple conversational turns:


          ```json

          [
            {"role": "user", "content": "Hello there."},
            {"role": "assistant", "content": "Hi, I'm Claude. How can I help you?"},
            {"role": "user", "content": "Can you explain LLMs in plain English?"},
          ]

          ```


          Example with a partially-filled response from Claude:


          ```json

          [
            {"role": "user", "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"},
            {"role": "assistant", "content": "The best answer is ("},
          ]

          ```


          Each input message `content` may be either a single `string` or an
          array of content blocks, where each block has a specific `type`. Using
          a `string` for `content` is shorthand for an array of one content
          block of type `"text"`. The following input messages are equivalent:


          ```json

          {"role": "user", "content": "Hello, Claude"}

          ```


          ```json

          {"role": "user", "content": [{"type": "text", "text": "Hello,
          Claude"}]}

          ```


          Starting with Claude 3 models, you can also send image content blocks:


          ```json

          {"role": "user", "content": [
            {
              "type": "image",
              "source": {
                "type": "base64",
                "media_type": "image/jpeg",
                "data": "/9j/4AAQSkZJRg...",
              }
            },
            {"type": "text", "text": "What is in this image?"}
          ]}

          ```


          We currently support the `base64` source type for images, and the
          `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types.


          See
          [examples](https://docs.anthropic.com/en/api/messages-examples#vision)
          for more input examples.


          Note that if you want to include a [system
          prompt](https://docs.anthropic.com/en/docs/system-prompts), you can
          use the top-level `system` parameter — there is no `"system"` role for
          input messages in the Messages API.
        paramKey: messages
        in: body
        schema:
          type: array
          items:
            type: any
        required: true
        example:
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: system
        description: >-
          System prompt.


          A system prompt is a way of providing context and instructions to
          Claude, such as specifying a particular goal or role. See our [guide
          to system prompts](https://docs.anthropic.com/en/docs/system-prompts).
        paramKey: system
        in: body
        schema:
          type: any
        required: false
        example:
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: model
        description: The model that will complete your prompt.\n\nSee
          [models](https://docs.anthropic.com/en/docs/models-overview) for
          additional details and options.
        paramKey: model
        in: body
        schema:
          type: any
        required: true
        example:
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
    requestBodyType: json
  - moduleName: Messages / Create Beta
    method: post
    path: /v1/messages?beta=true
    description: >-
      Send a structured list of input messages with text and/or image content,
      and the model will generate the next message in the conversation.


      The Messages API can be used for either single queries or stateless
      multi-turn conversations.
    externalDocs: ""
    params:
      - paramName: anthropicBeta
        description: >-
          Optional header to specify the beta version(s) you want to use.


          To use multiple betas, use a comma separated list like `beta1,beta2`
          or specify the header multiple times for each beta.
        paramKey: anthropic-beta
        in: header
        schema:
          type: string
        required: false
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
      - paramName: model
        description: The model that will complete your prompt.\n\nSee
          [models](https://docs.anthropic.com/en/docs/models-overview) for
          additional details and options.
        paramKey: model
        in: body
        schema:
          type: any
        required: true
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: messages
        description: >-
          Input messages.


          Our models are trained to operate on alternating `user` and
          `assistant` conversational turns. When creating a new `Message`, you
          specify the prior conversational turns with the `messages` parameter,
          and the model then generates the next `Message` in the conversation.
          Consecutive `user` or `assistant` turns in your request will be
          combined into a single turn.


          Each input message must be an object with a `role` and `content`. You
          can specify a single `user`-role message, or you can include multiple
          `user` and `assistant` messages.


          If the final message uses the `assistant` role, the response content
          will continue immediately from the content in that message. This can
          be used to constrain part of the model's response.


          Example with a single `user` message:


          ```json

          [{"role": "user", "content": "Hello, Claude"}]

          ```


          Example with multiple conversational turns:


          ```json

          [
            {"role": "user", "content": "Hello there."},
            {"role": "assistant", "content": "Hi, I'm Claude. How can I help you?"},
            {"role": "user", "content": "Can you explain LLMs in plain English?"},
          ]

          ```


          Example with a partially-filled response from Claude:


          ```json

          [
            {"role": "user", "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"},
            {"role": "assistant", "content": "The best answer is ("},
          ]

          ```


          Each input message `content` may be either a single `string` or an
          array of content blocks, where each block has a specific `type`. Using
          a `string` for `content` is shorthand for an array of one content
          block of type `"text"`. The following input messages are equivalent:


          ```json

          {"role": "user", "content": "Hello, Claude"}

          ```


          ```json

          {"role": "user", "content": [{"type": "text", "text": "Hello,
          Claude"}]}

          ```


          Starting with Claude 3 models, you can also send image content blocks:


          ```json

          {"role": "user", "content": [
            {
              "type": "image",
              "source": {
                "type": "base64",
                "media_type": "image/jpeg",
                "data": "/9j/4AAQSkZJRg...",
              }
            },
            {"type": "text", "text": "What is in this image?"}
          ]}

          ```


          We currently support the `base64` source type for images, and the
          `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types.


          See
          [examples](https://docs.anthropic.com/en/api/messages-examples#vision)
          for more input examples.


          Note that if you want to include a [system
          prompt](https://docs.anthropic.com/en/docs/system-prompts), you can
          use the top-level `system` parameter — there is no `"system"` role for
          input messages in the Messages API.
        paramKey: messages
        in: body
        schema:
          type: array
          items:
            type: any
        required: true
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: maxTokens
        description: >-
          The maximum number of tokens to generate before stopping.


          Note that our models may stop _before_ reaching this maximum. This
          parameter only specifies the absolute maximum number of tokens to
          generate.


          Different models have different maximum values for this
          parameter.  See
          [models](https://docs.anthropic.com/en/docs/models-overview) for
          details.
        paramKey: max_tokens
        in: body
        schema:
          type: number
          minimum: 1
        required: true
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: metadata
        description: An object describing metadata about the request.
        paramKey: metadata
        in: body
        schema:
          type: any
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: stopSequences
        description: >-
          Custom text sequences that will cause the model to stop generating.


          Our models will normally stop when they have naturally completed their
          turn, which will result in a response `stop_reason` of `"end_turn"`.


          If you want the model to stop generating when it encounters custom
          strings of text, you can use the `stop_sequences` parameter. If the
          model encounters one of the custom sequences, the response
          `stop_reason` value will be `"stop_sequence"` and the response
          `stop_sequence` value will contain the matched stop sequence.
        paramKey: stop_sequences
        in: body
        schema:
          type: array
          items:
            type: any
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: stream
        description: >-
          Whether to incrementally stream the response using server-sent events.


          See [streaming](https://docs.anthropic.com/en/api/messages-streaming)
          for details.
        paramKey: stream
        in: body
        schema:
          type: boolean
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: system
        description: >-
          System prompt.


          A system prompt is a way of providing context and instructions to
          Claude, such as specifying a particular goal or role. See our [guide
          to system prompts](https://docs.anthropic.com/en/docs/system-prompts).
        paramKey: system
        in: body
        schema:
          type: any
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: temperature
        description: >-
          Amount of randomness injected into the response.


          Defaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature`
          closer to `0.0` for analytical / multiple choice, and closer to `1.0`
          for creative and generative tasks.


          Note that even with `temperature` of `0.0`, the results will not be
          fully deterministic.
        paramKey: temperature
        in: body
        schema:
          type: number
          minimum: 0
          maximum: 1
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: toolChoice
        description: How the model should use the provided tools. The model can use a
          specific tool, any available tool, or decide by itself.
        paramKey: tool_choice
        in: body
        schema:
          type: any
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: tools
        description: >-
          Definitions of tools that the model may use.


          If you include `tools` in your API request, the model may return
          `tool_use` content blocks that represent the model's use of those
          tools. You can then run those tools using the tool input generated by
          the model and then optionally return results back to the model using
          `tool_result` content blocks.


          Each tool definition includes:


          * `name`: Name of the tool.

          * `description`: Optional, but strongly-recommended description of the
          tool.

          * `input_schema`: [JSON schema](https://json-schema.org/) for the tool
          `input` shape that the model will produce in `tool_use` output content
          blocks.


          For example, if you defined `tools` as:


          ```json

          [
            {
              "name": "get_stock_price",
              "description": "Get the current stock price for a given ticker symbol.",
              "input_schema": {
                "type": "object",
                "properties": {
                  "ticker": {
                    "type": "string",
                    "description": "The stock ticker symbol, e.g. AAPL for Apple Inc."
                  }
                },
                "required": ["ticker"]
              }
            }
          ]

          ```


          And then asked the model "What's the S&P 500 at today?", the model
          might produce `tool_use` content blocks in the response like this:


          ```json

          [
            {
              "type": "tool_use",
              "id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
              "name": "get_stock_price",
              "input": { "ticker": "^GSPC" }
            }
          ]

          ```


          You might then run your `get_stock_price` tool with `{"ticker":
          "^GSPC"}` as an input, and return the following back to the model in a
          subsequent `user` message:


          ```json

          [
            {
              "type": "tool_result",
              "tool_use_id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
              "content": "259.75 USD"
            }
          ]

          ```


          Tools can be used for workflows that include running client-side tools
          and functions, or more generally whenever you want the model to
          produce a particular JSON structure of output.


          See our [guide](https://docs.anthropic.com/en/docs/tool-use) for more
          details.
        paramKey: tools
        in: body
        schema:
          type: array
          items:
            type: any
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: topK
        description: >-
          Only sample from the top K options for each subsequent token.


          Used to remove "long tail" low probability responses. [Learn more
          technical details
          here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).


          Recommended for advanced use cases only. You usually only need to use
          `temperature`.
        paramKey: top_k
        in: body
        schema:
          type: number
          minimum: 0
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: topP
        description: >-
          Use nucleus sampling.


          In nucleus sampling, we compute the cumulative distribution over all
          the options for each subsequent token in decreasing probability order
          and cut it off once it reaches a particular probability specified by
          `top_p`. You should either alter `temperature` or `top_p`, but not
          both.


          Recommended for advanced use cases only. You usually only need to use
          `temperature`.
        paramKey: top_p
        in: body
        schema:
          type: number
          minimum: 0
          maximum: 1
        required: false
        example:
          max_tokens: 1024
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
    requestBodyType: json
  - moduleName: Models / List Beta
    method: get
    path: /v1/models?beta=true
    description: >-
      List available models.


      The Models API response can be used to determine which models are
      available for use in the API. More recently released models are listed
      first.
    externalDocs: ""
    params:
      - paramName: beforeId
        description: ID of the object to use as a cursor for pagination. When provided,
          returns the page of results immediately before this object.
        paramKey: before_id
        in: query
        schema:
          type: string
        required: false
      - paramName: afterId
        description: ID of the object to use as a cursor for pagination. When provided,
          returns the page of results immediately after this object.
        paramKey: after_id
        in: query
        schema:
          type: string
        required: false
      - paramName: limit
        description: |-
          Number of items to return per page.

          Defaults to `20`. Ranges from `1` to `1000`.
        paramKey: limit
        in: query
        schema:
          type: number
          minimum: 1
          maximum: 1000
          default: 20
        required: false
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Models / Get Beta
    method: get
    path: /v1/models/{model_id}?beta=true
    description: >-
      Get a specific model.


      The Models API response can be used to determine information about a
      specific model or resolve a model alias to a model ID.
    externalDocs: ""
    params:
      - paramName: modelId
        description: Model identifier or alias.
        paramKey: model_id
        in: path
        schema:
          type: string
        required: true
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Message Batches / Create Beta
    method: post
    path: /v1/messages/batches?beta=true
    description: >-
      Send a batch of Message creation requests.


      The Message Batches API can be used to process multiple Messages API
      requests at once. Once a Message Batch is created, it begins processing
      immediately. Batches can take up to 24 hours to complete.
    externalDocs: ""
    params:
      - paramName: anthropicBeta
        description: >-
          Optional header to specify the beta version(s) you want to use.


          To use multiple betas, use a comma separated list like `beta1,beta2`
          or specify the header multiple times for each beta.
        paramKey: anthropic-beta
        in: header
        schema:
          type: string
        required: false
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
      - paramName: requests
        description: List of requests for prompt completion. Each is an individual
          request to create a Message.
        paramKey: requests
        in: body
        schema:
          type: array
          items:
            type: any
        required: true
    requestBodyType: json
  - moduleName: Message Batches / List Beta
    method: get
    path: /v1/messages/batches?beta=true
    description: List all Message Batches within a Workspace. Most recently created
      batches are returned first.
    externalDocs: ""
    params:
      - paramName: beforeId
        description: ID of the object to use as a cursor for pagination. When provided,
          returns the page of results immediately before this object.
        paramKey: before_id
        in: query
        schema:
          type: string
        required: false
      - paramName: afterId
        description: ID of the object to use as a cursor for pagination. When provided,
          returns the page of results immediately after this object.
        paramKey: after_id
        in: query
        schema:
          type: string
        required: false
      - paramName: limit
        description: |-
          Number of items to return per page.

          Defaults to `20`. Ranges from `1` to `1000`.
        paramKey: limit
        in: query
        schema:
          type: number
          minimum: 1
          maximum: 1000
          default: 20
        required: false
      - paramName: anthropicBeta
        description: >-
          Optional header to specify the beta version(s) you want to use.


          To use multiple betas, use a comma separated list like `beta1,beta2`
          or specify the header multiple times for each beta.
        paramKey: anthropic-beta
        in: header
        schema:
          type: string
        required: false
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Message Batches / Retrieve Beta
    method: get
    path: /v1/messages/batches/{message_batch_id}?beta=true
    description: This endpoint is idempotent and can be used to poll for Message
      Batch completion. To access the results of a Message Batch, make a request
      to the `results_url` field in the response.
    externalDocs: ""
    params:
      - paramName: messageBatchId
        description: ID of the Message Batch.
        paramKey: message_batch_id
        in: path
        schema:
          type: string
        required: true
      - paramName: anthropicBeta
        description: >-
          Optional header to specify the beta version(s) you want to use.


          To use multiple betas, use a comma separated list like `beta1,beta2`
          or specify the header multiple times for each beta.
        paramKey: anthropic-beta
        in: header
        schema:
          type: string
        required: false
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Message Batches / Delete Beta
    method: delete
    path: /v1/messages/batches/{message_batch_id}?beta=true
    description: >-
      Delete a Message Batch.


      Message Batches can only be deleted once they've finished processing. If
      you'd like to delete an in-progress batch, you must first cancel it.
    externalDocs: ""
    params:
      - paramName: messageBatchId
        description: ID of the Message Batch.
        paramKey: message_batch_id
        in: path
        schema:
          type: string
        required: true
      - paramName: anthropicBeta
        description: >-
          Optional header to specify the beta version(s) you want to use.


          To use multiple betas, use a comma separated list like `beta1,beta2`
          or specify the header multiple times for each beta.
        paramKey: anthropic-beta
        in: header
        schema:
          type: string
        required: false
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Message Batches / Cancel Beta
    method: post
    path: /v1/messages/batches/{message_batch_id}/cancel?beta=true
    description: >-
      Batches may be canceled any time before processing ends. Once cancellation
      is initiated, the batch enters a `canceling` state, at which time the
      system may complete any in-progress, non-interruptible requests before
      finalizing cancellation.


      The number of canceled requests is specified in `request_counts`. To
      determine which requests were canceled, check the individual results
      within the batch. Note that cancellation may not result in any canceled
      requests if they were non-interruptible.
    externalDocs: ""
    params:
      - paramName: messageBatchId
        description: ID of the Message Batch.
        paramKey: message_batch_id
        in: path
        schema:
          type: string
        required: true
      - paramName: anthropicBeta
        description: >-
          Optional header to specify the beta version(s) you want to use.


          To use multiple betas, use a comma separated list like `beta1,beta2`
          or specify the header multiple times for each beta.
        paramKey: anthropic-beta
        in: header
        schema:
          type: string
        required: false
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Message Batches / Get Results Beta
    method: get
    path: /v1/messages/batches/{message_batch_id}/results?beta=true
    description: >-
      Streams the results of a Message Batch as a `.jsonl` file.


      Each line in the file is a JSON object containing the result of a single
      request in the Message Batch. Results are not guaranteed to be in the same
      order as requests. Use the `custom_id` field to match results to requests.
    externalDocs: ""
    params:
      - paramName: messageBatchId
        description: ID of the Message Batch.
        paramKey: message_batch_id
        in: path
        schema:
          type: string
        required: true
      - paramName: anthropicBeta
        description: >-
          Optional header to specify the beta version(s) you want to use.


          To use multiple betas, use a comma separated list like `beta1,beta2`
          or specify the header multiple times for each beta.
        paramKey: anthropic-beta
        in: header
        schema:
          type: string
        required: false
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
  - moduleName: Messages / Count Tokens Beta
    method: post
    path: /v1/messages/count_tokens?beta=true
    description: >-
      Count the number of tokens in a Message.


      The Token Count API can be used to count the number of tokens in a
      Message, including tools, images, and documents, without creating it.
    externalDocs: ""
    params:
      - paramName: anthropicBeta
        description: >-
          Optional header to specify the beta version(s) you want to use.


          To use multiple betas, use a comma separated list like `beta1,beta2`
          or specify the header multiple times for each beta.
        paramKey: anthropic-beta
        in: header
        schema:
          type: string
        required: false
      - paramName: anthropicVersion
        description: >-
          The version of the Anthropic API you want to use.


          Read more about versioning and our version history
          [here](https://docs.anthropic.com/en/api/versioning).
        paramKey: anthropic-version
        in: header
        schema:
          type: string
        required: false
      - paramName: toolChoice
        description: How the model should use the provided tools. The model can use a
          specific tool, any available tool, or decide by itself.
        paramKey: tool_choice
        in: body
        schema:
          type: any
        required: false
        example:
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: tools
        description: >-
          Definitions of tools that the model may use.


          If you include `tools` in your API request, the model may return
          `tool_use` content blocks that represent the model's use of those
          tools. You can then run those tools using the tool input generated by
          the model and then optionally return results back to the model using
          `tool_result` content blocks.


          Each tool definition includes:


          * `name`: Name of the tool.

          * `description`: Optional, but strongly-recommended description of the
          tool.

          * `input_schema`: [JSON schema](https://json-schema.org/) for the tool
          `input` shape that the model will produce in `tool_use` output content
          blocks.


          For example, if you defined `tools` as:


          ```json

          [
            {
              "name": "get_stock_price",
              "description": "Get the current stock price for a given ticker symbol.",
              "input_schema": {
                "type": "object",
                "properties": {
                  "ticker": {
                    "type": "string",
                    "description": "The stock ticker symbol, e.g. AAPL for Apple Inc."
                  }
                },
                "required": ["ticker"]
              }
            }
          ]

          ```


          And then asked the model "What's the S&P 500 at today?", the model
          might produce `tool_use` content blocks in the response like this:


          ```json

          [
            {
              "type": "tool_use",
              "id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
              "name": "get_stock_price",
              "input": { "ticker": "^GSPC" }
            }
          ]

          ```


          You might then run your `get_stock_price` tool with `{"ticker":
          "^GSPC"}` as an input, and return the following back to the model in a
          subsequent `user` message:


          ```json

          [
            {
              "type": "tool_result",
              "tool_use_id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
              "content": "259.75 USD"
            }
          ]

          ```


          Tools can be used for workflows that include running client-side tools
          and functions, or more generally whenever you want the model to
          produce a particular JSON structure of output.


          See our [guide](https://docs.anthropic.com/en/docs/tool-use) for more
          details.
        paramKey: tools
        in: body
        schema:
          type: array
          items:
            type: any
        required: false
        example:
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: messages
        description: >-
          Input messages.


          Our models are trained to operate on alternating `user` and
          `assistant` conversational turns. When creating a new `Message`, you
          specify the prior conversational turns with the `messages` parameter,
          and the model then generates the next `Message` in the conversation.
          Consecutive `user` or `assistant` turns in your request will be
          combined into a single turn.


          Each input message must be an object with a `role` and `content`. You
          can specify a single `user`-role message, or you can include multiple
          `user` and `assistant` messages.


          If the final message uses the `assistant` role, the response content
          will continue immediately from the content in that message. This can
          be used to constrain part of the model's response.


          Example with a single `user` message:


          ```json

          [{"role": "user", "content": "Hello, Claude"}]

          ```


          Example with multiple conversational turns:


          ```json

          [
            {"role": "user", "content": "Hello there."},
            {"role": "assistant", "content": "Hi, I'm Claude. How can I help you?"},
            {"role": "user", "content": "Can you explain LLMs in plain English?"},
          ]

          ```


          Example with a partially-filled response from Claude:


          ```json

          [
            {"role": "user", "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"},
            {"role": "assistant", "content": "The best answer is ("},
          ]

          ```


          Each input message `content` may be either a single `string` or an
          array of content blocks, where each block has a specific `type`. Using
          a `string` for `content` is shorthand for an array of one content
          block of type `"text"`. The following input messages are equivalent:


          ```json

          {"role": "user", "content": "Hello, Claude"}

          ```


          ```json

          {"role": "user", "content": [{"type": "text", "text": "Hello,
          Claude"}]}

          ```


          Starting with Claude 3 models, you can also send image content blocks:


          ```json

          {"role": "user", "content": [
            {
              "type": "image",
              "source": {
                "type": "base64",
                "media_type": "image/jpeg",
                "data": "/9j/4AAQSkZJRg...",
              }
            },
            {"type": "text", "text": "What is in this image?"}
          ]}

          ```


          We currently support the `base64` source type for images, and the
          `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types.


          See
          [examples](https://docs.anthropic.com/en/api/messages-examples#vision)
          for more input examples.


          Note that if you want to include a [system
          prompt](https://docs.anthropic.com/en/docs/system-prompts), you can
          use the top-level `system` parameter — there is no `"system"` role for
          input messages in the Messages API.
        paramKey: messages
        in: body
        schema:
          type: array
          items:
            type: any
        required: true
        example:
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: system
        description: >-
          System prompt.


          A system prompt is a way of providing context and instructions to
          Claude, such as specifying a particular goal or role. See our [guide
          to system prompts](https://docs.anthropic.com/en/docs/system-prompts).
        paramKey: system
        in: body
        schema:
          type: any
        required: false
        example:
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
      - paramName: model
        description: The model that will complete your prompt.\n\nSee
          [models](https://docs.anthropic.com/en/docs/models-overview) for
          additional details and options.
        paramKey: model
        in: body
        schema:
          type: any
        required: true
        example:
          messages:
            - content: Hello, world
              role: user
          model: claude-3-5-sonnet-20241022
    requestBodyType: json
